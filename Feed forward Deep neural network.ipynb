{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fanibuyanoluwaseyi/Mytestrepo/blob/main/Image_processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "feed forward deep neural network that classify images from the Fashion-MNIST\n",
        "dataset. Used the RELU activation function and the Adam optimiser. Changed the other parameters of  the network and hyperparameters to achieve the best accuracy."
      ],
      "metadata": {
        "id": "62xVuzh7-zBO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vB2x2U_0HSIA"
      },
      "outputs": [],
      "source": [
        "#import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import statsmodels.api as sm\n",
        "import itertools\n",
        "import sklearn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #load the mnist dataset from keras\n",
        "import tensorflow as tf\n",
        "from keras import datasets\n",
        "from tensorflow.keras import datasets\n",
        "(x_train, y_train), (x_test, y_test) = datasets.fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "LHc5P8YQHiYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the MNIST dataset\n",
        "from keras.datasets import mnist\n",
        "(x_train, y_train), (x_test, y_test) = datasets.fashion_mnist.load_data()\n",
        "\n",
        "# Normalize pixel values between 0 and 1\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# One hot encode the output data\n",
        "from keras.utils import to_categorical\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)"
      ],
      "metadata": {
        "id": "0Y5dN31FHpIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Create, compile and fit model with original MNIST data only**"
      ],
      "metadata": {
        "id": "oQxi0WKcHqLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "reg = regularizers.L2(0.001)\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Flatten(input_shape=(28, 28)),\n",
        "    layers.Dense(512, activation='relu', kernel_regularizer=reg),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(256, activation='relu', kernel_regularizer=reg),\n",
        "    layers.Dropout(0.4),\n",
        "    layers.Dense(128, activation='relu', kernel_regularizer=reg),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(x_train, y_train, batch_size=256, epochs=100, verbose=1, validation_data=(x_test, y_test), callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)])\n",
        "\n",
        "\n",
        "# plot the history of the training\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# summarize the history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train','test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# summarize the history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train','test'], loc='upper left')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "w3UO0FYwHteD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reduced the L2 regularization to 0.001 to avoid overfitting.\n",
        "\n",
        "Increased the number of neurons in the first layer to 512, second layer to 256, and third layer to 128.\n",
        "\n",
        "Increased the dropout rate in the first layer to 0.5, 0.4 for the second layer and 0.3 for he third layer.\n",
        "\n",
        "Added an early stopping callback to stop training when the validation loss stops improving.\n",
        "\n",
        "Application of learning rate of 0.0001"
      ],
      "metadata": {
        "id": "SHI0fyq5KKsV"
      }
    }
  ]
}
